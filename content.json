{"pages":[{"title":"About","text":"Welcome.My name is Jeffrey Luppes. For most of my online presence I use the handle @jeffluppes, mainly because I like it more - there’s also another individual with the same name in the US and this helps people tell us apart - I am not a professor of the German Language. Sorry to dissapoint! I am currently enrolled in the Computing Science Data Science Master at the Radboud University Nijmegen. Previously I did a bachelor degree in Software Engineering at Groningen (Hanzehogeschool Groningen) and my bachelor thesis at TNO. I am hoping to graduate in the winter of 2018. I have a high interest in data science applications, particularly when they may have an impact on society and better our lives. I come from a programming background and I credit having a more hands-on approach to it. You can reach out to me on my LinkedIn or via jeffluppes[at]gmail.com I also kayak a lot and love being outside - being on my ATB or hiking and enjoying the sight. You can even find me out on the North Sea but mostly anywhere on flatwater in the Netherlands. I am an assistant instructor and like to organize kayaking trips - either solo, or as a group leader. I also publish kayaking videos. My dream is to one day undertake long expeditions by kayak - such as circumnavigating Iceland or Ireland. I am currently paddling at KV Jason (Arnhem) and the Groninger Kanovereniging (Groningen). I paddle an old 1980s Valley Selkie in addition to various club boats (P&amp;H Cappellas, Scorpios and a Dawntreader Odin, and a Valley Avocet)","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"Kayaking","text":"","link":"/kayaking/index.html"}],"posts":[{"title":"An actual Hello World","text":"I just got Hexo up and running and it seems to work smoothly. I was pretty excited to find a blog based on node.js and coming from a world of Wordpress it was amusing that people actually ported Wordpress themes to Hexo. Normally I would have preferred a much more minimalistic skin, but as I hope to post about outdoors stuff too, I figured I could use some color in my theme. Why the hell did you choose Hexo though?I’ll admit it. I’m not someone who usually adopts software if there isn’t a great community supporting it. As Hexo‘s community is mostly Chinese (?) it could be hard to get English-language help, but configuring Hexo was easy and just a little bit of command line stuff. Editing is easy, too. And I couldn’t really find a reason not to go for it! Does the world really need another student building yet another blog?Perhaps not, but the further I get in my career the more important it gets to have something that can be googled. Plus: I like writing as much about code and the things I do as I do making things. Creativity needs to be let out for both processes. Why don’t you post in Dutch though?I guess I will eventually, it’d be gezellig. And what about your old Wordpress site?Wait, that’s still around?","link":"/2015/10/26/An-actual-Hello-World/"},{"title":"Big Data Series","text":"Big Data SeriesI’ve just added three blog posts I made during the Big Data bachelor course given at the Radboud university. As a master’s student I’m allowed to take on one or two bachelor courses if there’s a good reason… because no other course really goes into Spark, hadoop and Scala I figured it would be a nice addition to the Python-heavy curriculum. Not that I dislike Python, of course. There are three posts in total: Hadoop and the HDFS - an introduction to hadoop and HDFS.Spark - On looking at a Kaggle competition data set in SparkThe class project: A solo project about submitting code to a national research cluster and running queries against 1.73 billion web pages. You can find the posts here: Big Data Series I learnt a lot and finished the class project with a 9.5.","link":"/2017/09/02/Big-Data-Series/"},{"title":"Big Data Series - Give me a spark","text":"Assignment 3 - Give me a sparkSo the third assignment is running Spark and playing around with it. The first part was basically messing about with the query-processing, the second part is playing with data and dataframes. As these do not actually seem to be part of the required stuff for the post, I have left them out completely. The way I understand this is that I’m supposed to play with Spark, come up with something new, and write a short blog post detailing my experiences. Alternatively, you could decide to carry out a small analysis of a different open dataset, of your own choice; and present to your readers some basic properties of that data. You will notice that it is harder than following instructions, and you run the risk of getting stuck because the data does not parse well, etc. So without further ado, let’s explore some datas. Part 1 - Getting dataKaggle is one of the top resources for Data Science competitions, where data scientists, analysts, and programmers of all flavors unite and compete for prizes. While IMO prize money mostly seems to go to people who already have top-tier knowledge (like people who work at Deepmind) or just a lot of time/resources behind them (I recall reading some people spend 5 hours a day on a competition, which would probably make the pay-off very poor for their time investment), it’s kind of a data geek playground. I have selected the Sberbank competition which was launched recently. The first step is simply downloading the accepting the conditions of the competition, and downloading a zip file for training and test data. Additional data about Russia’s economy is available in different files, and the file description hints that these may be joined together with the proper instructions. All the data is in comma-seperated values. The training data is 44 MB. Because one cay only download the data once authenticated and I’m using a virtual machine, I’ve hosted the dataset elsewhere before pulling it in with wget as this seems an easier solution than setting up a shared partition. Part 2 - Importif you are interested in following along, use wget https://www.dropbox.com/s/4dmsg68lc509q32/train.csv?dl=0. I’ll make sure this file stays available for the next few weeks. Importing the data can be done with the same instructions as any other csv. val rusdata = spark.read.format(&quot;csv&quot;).option(&quot;header&quot;, true).load(&quot;notebooks/BigData/data/train.csv?dl=0&quot;).cache() With printSchema I can take a look at the schema of the data set. This also allows for verification that everything got loaded in. rusdata.printSchema Which outputs all the fields from the dataset as follows: root |-- id: string (nullable = true) |-- timestamp: string (nullable = true) |-- full_sq: string (nullable = true) |-- life_sq: string (nullable = true) |-- floor: string (nullable = true) |-- max_floor: string (nullable = true) |-- material: string (nullable = true) |-- build_year: string (nullable = true) |-- num_room: string (nullable = true) |-- kitch_sq: string (nullable = true) |-- state: string (nullable = true) |-- product_type: string (nullable = true) |-- sub_area: string (nullable = true) |-- area_m: string (nullable = true) |-- raion_popul: string (nullable = true) |-- green_zone_part: string (nullable = true) |-- indust_part: string (nullable = true) |-- children_preschool: string (nullable = true) |-- preschool_quota: string (nullable = true) |-- preschool_education_centers_raion: string (nullable = true) .... rusdata: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, timestamp: string ... 290 more fields] 292 fields. Not bad! Part 3 - PlayingSo now let’s explore the data! How does a property object look like, actually? rusdata.show(1) So it’s just a single row in this massive csv. Each house has an ID and 290 other properties that go with it. Finally, the last property is the house price itself, which is used in the competition mainly. Something that catches my eye in both of these is the large amount of features that have a suffix of 500, 1000 or 2000. This could be how many of that particular feature are in a 500 or 1000 meter radius around the object. Something that is also peculiar is that there are no latitude or longtitude pairs (be it WGS or a Russian format, neither appear) instead the highest “resolution” is the area in which the property is located. E.g in the ‘Kremlin’ area. This makes data sort of anonymous, but I suspect it would not be hard to identify property objects based on the 292 features that we have if we chose to do this. So the first thought.. can we find data on the Kremlin itself? From wikipedia I find that the Grand Kremlin Palace was built between 1837 and 1849 and has a sqaura area of 125 metres long, 47 metres high, and has a total area of about 25,000 square metres. It includes the earlier Terem Palace, nine churches from the 14th, 16th, and 17th centuries, the Holy Vestibule, and over 700 rooms Can we find that? val potentialKremlin = rusdata.select(&quot;id&quot;,&quot;full_sq&quot;,&quot;life_sq&quot;,&quot;floor&quot;,&quot;build_year&quot;,&quot;num_room&quot;).where(&quot;build_year &lt;= 1849&quot;) potentialKremlin.count 904 Now let’s filter this again, as we know at that it has 700 rooms.. can we reduce the set of 904 objects to something we can count on our fingers? Let’s filter this by selecting objects with more than 10 rooms. val potentialKremlin2 = potentialKremlin.select(&quot;id&quot;,&quot;full_sq&quot;,&quot;life_sq&quot;,&quot;floor&quot;,&quot;build_year&quot;,&quot;num_room&quot;).where(&quot;num_room &gt;= 10&quot;) potentialKremlin2.show() +---+-------+-------+-----+----------+--------+ | id|full_sq|life_sq|floor|build_year|num_room| +---+-------+-------+-----+----------+--------+ +---+-------+-------+-----+----------+--------+ So it seems that the Kremlin is not in the data set, despite being in the Kremlin district. Just to check - are there any properties with more than ten rooms in the data set at all? rusdata.select(&quot;id&quot;,&quot;full_sq&quot;,&quot;life_sq&quot;,&quot;floor&quot;,&quot;build_year&quot;,&quot;num_room&quot;).where(&quot;num_room &gt;= 10&quot;).show() +-----+-------+-------+-----+----------+--------+ | id|full_sq|life_sq|floor|build_year|num_room| +-----+-------+-------+-----+----------+--------+ |11624| 40| 19| 17| 2011| 19| |17767| 58| 34| 1| 1992| 10| |26716| 51| 30| 14| 1984| 17| |29175| 59| 33| 20| 2000| 10| +-----+-------+-------+-----+----------+--------+ It seems that having tons of rooms is simply a more modern fad. Now, let’s do some aggregation. Can we get a list of how many properties there are per region? This time we use SQL directly, but we need a dataframe first: val rusdataDF = rusdata.select(&quot;id&quot;,&quot;full_sq&quot;,&quot;life_sq&quot;,&quot;floor&quot;,&quot;build_year&quot;,&quot;num_room&quot;, &quot;sub_area&quot;) rusdataDF.createOrReplaceTempView(&quot;props&quot;) spark.sql(&quot;SELECT sub_area, count(sub_area) AS sa FROM props GROUP BY sub_area ORDER BY sa DESC&quot;).show(15) +--------------------+----+ | sub_area| sa| +--------------------+----+ | Poselenie Sosenskoe|1776| | Nekrasovka|1611| |Poselenie Vnukovskoe|1372| |Poselenie Moskovskij| 925| |Poselenie Voskres...| 713| | Mitino| 679| | Tverskoe| 678| | Krjukovo| 518| | Mar&apos;ino| 508| |Poselenie Filimon...| 496| | Juzhnoe Butovo| 451| |Poselenie Shherbinka| 443| | Solncevo| 421| | Zapadnoe Degunino| 410| |Poselenie Desjono...| 362| +--------------------+----+ only showing top 15 rows Note: the sub_areas are supposed to be the 125 raions of Moscow. But are they, actually? spark.sql(&quot;SELECT DISTINCT sub_area FROM props&quot;).count 146 So we have more areas than there are raions. This means we should be distrustful of the data: it could at the very least mean that we cannot expect all 125 areas to be present in the data set. One of the things we might expect to find is negative or zero entries in the build_year column. Can we find these with a simple SQL statement? spark.sql(&quot;SELECT id, build_year FROM props ORDER BY build_year ASC&quot;).show +-----+----------+ | id|build_year| +-----+----------+ |11010| 0| |12811| 0| |11186| 0| |10145| 0| +-----+----------+ uh oh.. spark.sql(&quot;SELECT id, build_year FROM props WHERE build_year == 0&quot;).count 530 spark.sql(&quot;SELECT id, build_year FROM props WHERE build_year &gt;= 2017&quot;).count 157 So at the very least there’s something funny going on with the building year data. But it could also mean that these building are still being built. A quick look at Kaggle’s forum gives the following answer: What 0 and 1 mean in ‘build_year” column of the data? These are mistakes in the raw data, so we cannot fix it, unfortunately. And what about the houses that are being built or have been built after the data was collected (as the data is from 2015) it could be pre-investment (see product type). Part 4 - FindingsI have added most of the queries I processed in the blog text above, but it would be really nice if I could host the spark notebook somewhere like I would with a ipython notebook. Overall I’m okay with looking at the data with Spark. For analysis though, I would use Python as it’s more established and I can get more support. Additionally, there are a lot of packages available that make development so much more doable. I did like working with Spark. The SQL-rich syntax makes it easy to learn, and the things that I found gave me plenty of ammunition to start work in node, R or Python.","link":"/2017/05/04/Give-me-a-spark/"},{"title":"De Vechtrally, in twee dagen 110 kilometer op de Vecht(e)","text":"De Vecht Ik liep al een tijdje met het idee rond om de Vecht af te varen. Ik ben er vlakbij opgegroeid en was wel benieuwd hoe de rivier van Duitsland bij het IJsselmeer uitkomt. Via Peddelpraat heb ik toen Ton (waterTon) een berichtje gestuurd over mijn plan, en met de vraag vanaf waar ik kon instappen. Ik wilde namelijk zo ver mogelijk stroomopwaarts beginnen. Kon ik verder gaan dan Nordhorn? Misschien zelfs bij Schüttorf in de buurt beginnen? Let op: deze post bevat veel afbeeldingen en het kan een tijdje duren voordat alles is ingeladen. Voor grotere afbeeldingen kun je klikken op de foto’s. Waterton en waterTonTon is een bekende in de Nederlandse Kajakscene. Hij heeft een eigen website waarop hij zijn avonturen publiceert. Vanaf zijn woonplaats aan de Vecht gaat hij regelmatig eropuit en heeft - op het moment dat ik hem benader - al bijna 10.000 kilometer op de Vecht gevaren in zijn boot - een ShoreLine genaamd Waterton. Na contact gaat hij voor mij op onderzoek uit in hoeverre hij stroomopwaarts kan varen vanaf Nordhorn. Zoals een kajakker betaamt betekent dit: proefvaren. Het blijkt dat het niet mogelijk is om verder stroomopwaarts te starten dan Nordhorn, aan de Vechtsee. Ton voorziet mij verder van een lijst met afstanden en kennis over obstakels en stroming (of eerder gebrek aan) onderweg. Een paar dagen voor mijn vertrek vraag ik of hij een stukje mee wil varen en hij gaat er op in. Zonder zijn ervaring zou ik dit verhaal waarschijnlijk nu niet uittypen. PlanningIn het weekend van de Veluwerally lijkt het zover te komen. Als student heb ik niet altijd tijd en ook niet echt altijd de luxe van vervoer: ik bezit zelf geen auto en zolang heb ik mijn rijbewijs nou ook nog weer niet. Het blijkt ook bijzonder mooi weer te worden: de verwachting is zonnig en rond de 20 graden. Mijn moeder biedt aan om het shuttelen te doen- het heen en weer rijden met kano naar begin- en eindpunt. Als planning maak ik een aantal kaarten met behulp van OpenSeaMap. Dit is een gratis applicatie en eigenlijk gewoon de kaartlaag van OpenStreetMap. Ik bestudeer verder Google Maps op mogelijke stroomversnellingen en steigers. Met deze informatie zet ik een “float plan” op, wat eigenlijk gewoon een relatieve afstandlijst is met alle punten die ik onderweg tegenkom. Het voordeel hiervan is dat het redelijk inzichtelijk is waar ik ongeveer ben op een bepaald moment. Het thuisfront heeft ook een kopie en kan zo bekijken hoe mijn dag eruit ziet. De voornaamste rustpunten zijn de stuwen en sluizen die ik tegenkom. Tot slot kijk ik naar de lokale flora en fauna via gerichte zoekopdrachten en Waarneming.nl. Voornamelijk om mijn oren en ogen open te houden voor otters en bevers… en het is ook een beetje voorpret. Op donderdagavond na mijn laatste college haal ik met mijn moeder mijn kano op. Met nog even een dagje rust op vrijdag om spullen in orde te maken en wat huiswerk weg te werken. Mijn spullen zet ik meestal van te voren op een foto: dit zorgt ervoor dat ik makkelijk kan zien of ik iets kwijt of vergeten ben. De avond van tevoren ga ik redelijk vroeg naar bed (12 uur ‘s nachts?) met de tassen in de gang. Alle lichten staan op groen om morgen te vertrekken…. Dag 1 - Schildpadden, mildwater en schrammen op de bootMijn zaterdag begint vroeg. Rond 6 uur ben ik de kano op de oude Ford aan het monteren. Ik gebruik hiervoor een opblaasbaar Handirack: een prachtuitvinding ware het niet dat het ontzettend lang duurt om iets goed te zekeren op de Handirack. Na een koffie vertrekken we in lichte mist die tot in Duitsland aanhoudt. Eenmaal in Nordhorn kan ik zoals verwacht gemakkelijk instappen aan de Vechtsee. De auto staat op een parkeerplaats bij een hotel en het is amper 20 meter lopen. Toch word het al snel negen uur… Op de Vechtsee kan ik een beetje genieten van de romantiek. Een lieflijk bruggetje, mooie bomen en veel joggers die genieten van hun vrije dag terwijl de stad Nordhorn langzaam ontwaakt. Toch schiet ik snel verder Nordhorn in. De Vechte is hier ongeveer zoals je zou verwachten van een stadsrivier. Veel rotzooi, veel dode vis. Ik kom aan bij de eerste overdraging: de kanogoot van Nordhorn. Op zich zou hij wel te doen zijn, maar in mijn 30 jaar oude Selkie die ook nog eens vol beladen is wil ik het risico niet nemen. Ik til de boot uit het water en leun met mijn volle gewicht op de bosjes terwijl ik de kano voorbij de rimboe probeer te krijgen. Hier moet nodig gesnoeid worden. Eenmaal weer in het water merk ik dat de Vechte hier absoluut niet diep is. Soms gaat het water met een kleine portie geweld tussen de obstakels door. Dit is mildwater- niet wildwater, maar ook absoluut niet lekker om te doen in een polyester zeekano. Ik schuur met de kajak over de bodem en bij een enkele punt raak ik stenen. Thuis maar controleren hoe erg het is.. denk ik, terwijl ik ook maar even na ga in welk luik ik de rol duct tape heb gegooid.., Overigens ruikt de Vechte hier bijzonder sterk zoals de Rijn en IJssel kunnen ruiken: een beetje een steenig geurtje dat doet denken aan een kelderluchtje. Eenmaal uit Nordhorn is het water vlak. Het is windstil en enkele fietsers zeggen hallo. “Nach Holland?” vraagt eentje. “Ja, nach Holland!” roep ik terug. Ik zie er waarschijnlijk uit als een toerist. Britse vlag op mijn boot en spatzeil, Amerikaanse vlag prominent op de peddel. Al mijn uitrusting heeft kleur en ik steek af tegen de natuurlijke rivier. Ik ben het Nederlandse equivalent van de Duitse fietstoerist op de Waddeneilanden. Ook mijn zwemvest heb ik natuurlijk niet per sé nodig. Maar als ik alleen ben neem ik liever wat extra veiligheidsmarges – en ook passen mijn mesje, telefoon, water en eten en ontelbaar meer dingen in mijn vest. Een stukje verderop kom ik bij een nieuwe bypass. Vreemd, deze stond nog niet op de kaart. Ik passeer door een oerwoud aan boomstronken en kom vast te liggen tussen de rotzooi van van Nordhorn: dikke frisdrankpakken en flessen. Met moeite worstel ik me vrij en begin weer te varen. Plots zie ik iets vreemds op een boomstronk voor me: een oranje onderbuikje, een dekschildje en een ongelofelijk chagrijnige kop kijkt mij aan voordat hij het water in glipt: een schildpad zat heerlijk te zonnen. Dat zie je niet altijd. Toen kon ik nog niet weten dat ik de volgende dag er nog een zou spotten… Bij Neuenhaus mag ik er weer uit. Hier is een stuw en het is te merken dat het water erg laag staat: de kanosteiger hangt zo’n meter boven mij en de kant is dicht begroeid. Met een tactisch peddelbruggetje kom ik aan land en sleep ik mijn kano voor. Hier was ook een kanogoot geweest, maar in mijn oude boot wil ik de schade niet riskeren. Een goed plan blijkt later: de kanogoot staat op het einde een beetje droog. Moet je maar weten. Ik vaar verder en passeer ook de derde uitstap: een stuw bij de waterzuivering van Neuenhaus. Hier blijkt het omlopen en overtillen een hele kluif: ze mogen die steigers wel dichterbij de stuw plaatsen wat mij betreft. Niet lang daarna is nummer vier aan de beurt: een steile helling en een diep kanaal zorgen ook hier voor een challenge factor. Helemaal uitgeblust pak ik mijn pauze hier. Ik zou hier om 12:00 al willen aankomen, het is nu iets over 13:00. De kanosteiger blijkt weggehaald te zijn – mogelijk door de Vechte zelf. Ik stap in via een soort van takelvoorziening die is aangelegd rondom de stuw, misschien voor onderhoud. Hier eindelijk een beetje stroming en ik ga als een speer door richting Hoogstede. Nauwelijks een kwartier later kom ik Ton tegen! Ik had gedacht dat Ton pas een uur voor het middaguur op zou stappen maar hij blijkt al eerder dan mij te water zijn gegaan. Ik had natuurlijk al een redelijke vertraging, en zo komt het dat hij mij pas hier treft. We schudden elkaar de hand en leggen de ontmoeting vast. Ton is een behendige vaarder en veel vlugger dan je gezien zijn leeftijd zou verwachten. Hij vaart op techniek en heeft hierin ook een beetje zijn eigen didactiek ontwikkeld. Zodoende krijg ik al snel tips van hem en probeer ik deze, al varende, toe te passen. Ook al ben ik zelf ondertussen assistent-instructeur steek ik best wel wat op. Gezien dat Ton mij geneigd is voorbij te gaan (ook al varen we volgens mijn GPS riant tussen de 7.5 en 8 km per uur) is het een effectieve manier van varen. Onderweg biedt hij mij aan om bij hem en zijn vrouw te eten en te logeren. Ik had geplanned om bij de zorgboerderij aan de Loozensche Linie te kamperen, vlakbij het water. Na wat twijfelen (ben ik jullie dan niet tot last?) ging ik er op in. Ton gaat ook zorgen dat mijn kajak ’s ochtends weer naar het water gebracht kan worden. Al pratend bereiken we Laar, het laatste Duitse dorpje op ongeveer 40 kilometer. Laar is best wel Nederlands: een molen staat prominent in het dorpsbeeld en het dorpje ziet er netjes uit - niet dat de meeste Duitse dorpjes dat niet zijn. Ton dirigeert mij naar de rechteroever en door de rietkragen komen we op een beschut meertje terecht. Hier veel planten en eenden, een perfecte plek om op een zonnige middag een boek te lezen of te picknicken. Dan moet je natuurlijk wel een kano hebben ;) We steken de grens over naar Nederland. Hier is het een beetje een desolate bedoeling: de Vecht is hier “werk in uitvoering” en de graafmachines zijn nog aan het werk. We varen door naar het sluis/stuw/mini-waterkrachtcentralecomplex van de Haandrik voor de laatste overstap. Het is nog een klein stukje naar De Loozensche Linie waar Ton zijn auto heeft staan. We komen rond 19:00 bij Ton en Berna’s huis aan en mijn kano krijgt een plekje in de garagebox. Ik mag mee-eten en gebruikmaken van de logeerkamer. Heerlijk! Veel beter dan een natte tent morgenochtend. Dag 2 - Mist, vistrappen en het verhaal van een duifDag 2 begint met een lekker ontbijt van Ton en rond half 9 word ik weer afgezet bij het uitstappunt van gisteravond. Het is extreem mistig: perfect kanoweer: ik hoef mij niet in te smeren en ook niet na te denken over moeilijke zaken. Ik hang lekker de fotograaf uit in het mistige wereldje. Koeien, Canadese Ganzen en spinnewebben passeren mijn lens. Bij Hardenberg (5 kilometer) staat Ton mij op te wachten: hij schiet nog wat foto’s terwijl ik de kano door de poortjes van de vistrappen bij Hardenberg dirigeer. Ondanks dat geen van dit alles hiervoor bedoeld is, gaat het prima met de zeekano. Mijn grijns is na afloop breder dan sommige doorgangen waar ik de kano doorheen jaag. Vlak voordat Ton en ik afscheid nemen vliegen twee ijsvogeltjes rakelings langs mijn boot. Ja, die nestelen hier. Ik zal de rest van de dag nog een stuk of 12-13 tegenkomen. Voorbij Hardenberg neemt de mist weer toe. Op een eilandje ontstaan bij de herintrede van een meander van de Vecht kom ik twee wildkamperende canadeesvaarders tegen. Ze hebben muziek aan en hebben mij geeneens door als ik vlakbij in het water langs hun glijd. Bij de eerste stuw die ik daarna tegenkom zit een mooie bypass waaraan geen einde lijkt te komen tot ik op een vistrap stuit. Ik zou hier wel het water uit kunnen, maar ik moet erkennen dat de voorzieningen daarvoor wel wat karig zijn. Bij gebrek aan verbodsbord besluit ik over de vistrap te gaan, na eerst grondig de kat uit de boom te hebben gekeken. Mijn boot mag immers de houten vistrap niet beschadigen, en vice-versa. Het is een leuk parcours naar beneden en ik glij in een duiker vol met spinnenwebben. Dikke vette kruisspinnen hangen boven het water en lijken goed te gedijen op de aanwezige moerasmugjes. Vanaf dit moment neemt de mist af en wordt de lucht boven mij lichtblauw. Toch maar even insmeren dus. Ik zit nu een paar kilometer voor de stuw bij Junne ten hoogte van Mariënberg en Diffelen. Voor mij zie ik de bossen die ik een paar weken eerder met een ATB heb ontdekt. Hier stikt het van de campings en ik zit lekker in mijn flow.. de 8 kilometer per uur wordt weer behaald. Bij de stuw zitten twee vissers op de kanosteiger die graag een praatje maken. Ik til mijn kano uit het water en werp nog een blik op de “gevaarlijke” vistrap naast de stuw. Tja, misschien een andere keer. Aan de andere kant zitten ook twee vissers maar die negeren mij compleet. Ik besluit dan maar even te lunchen en straks maar aan de kade in te stappen. Vissers en kanoërs gaan op zich wel samen, maar een steiger blijft betwist gebied. Vlakbij de stuw kom ik een bootje tegen van een basisschoolleerling, met het verzoek aan de vinder(s) om een mailtje te sturen en de boot weer te water te laten. Nostalgie: ik deed dit ook vaak ook als kind. Luca, je bootje is weer op weg! In Ommen is het druk op het water. Er is een feest aan de gang van RTV Oost en er zijn veel bootjes op het water. Een passant vraagt of zijn sloep niet te sterke golven veroorzaakt. Nou nee hoor meneer, hij [de kano] kan ook de Noordzee op. Al ziet de beladen Selkie er een beetje uit als een onderzeeër momenteel.Tussen Ommen en Dalfsen passeer ik weer een stuw. Het stikt hier van de dagjesmensen, en het voelt niet vertrouwd om mijn kano te laten liggen om even ijs te halen bij de campings aan de overkant. Helaas. IJs moet ik maar inhalen. Ik stap weer in en ga varen, maar mijn motivatie kakt in. Plots zie ik weer iets wat niet op de vecht thuishoort… een schildpad zit hier relaxt te zonnen en blijft lekker liggen als ik voorbijdobber. Hij heeft in ieder geval een goede dag uitgekozen! In Dalfsen hou ik even een drijfpauze als ik een geklets hoor. Een duif lijkt vast te zitten aan het water en doet verwoede pogingen om we te vliegen. Met natte vleugels komt hij niet los van het water. Ik schep hem op met mijn peddel en plaats hem op mijn dek om maar eens mooi op de foto te zetten. Een duif. Wat moet ik met een duif? Het beestje lijkt door te hebben dat ik hem niks aandoe. Een paar heren van ongeveer mijn leeftijd hebben de reddingsactie gevolgd vanaf de kant en nemen de duif over met een schepnet en beloven er op te letten dat hij niet weer de Vecht intuimelt. Rond een uur of vijf kom ik aan bij de laatste stuw/sluiscombinatie van de Vecht. Hier is een kanosteiger maar ik neem een langere pauze. De teller staat nu op 45 kilometer en ik moet nog 15- die 15 wil ik eigenlijk in één ruk afleggen. Ik stap aan de andere kant weer het water in en zet de snelheid er weer flink in. Volgens de GPS vaar ik zelfs nu nog net ongeveer 8.5 kilometer per uur. Dat is ongeveer de maximale snelheid die de Selkie vol kan houden. Misschien staat er toch wat stroming, ik vertrouw echt niet dat ik zelf zo snel doorvaar. Ik kom al snel bij de spoorbrug naar Groningen en Friesland. Zelf ben ik ontelbare keren deze brug als treinreiziger gepasseerd. Met de ondergaande zon op mijn linkerflank zet ik de peddel er nog even in en hou ik rekening met een aankomst tussen half 8 en 8 in Hasselt. Rond 10 over 7 kom ik op het Zwarte water uit: dit is het eindpunt van de Vecht. Nu nog even doorzetten richting Hasselt. Er is vrijwel niemand op het water: enkel een visser die ook even moet controleren wat hij voor zich ziet. De stadssluisdeuren van Hasselt staan open en ik vaar zo de binnengracht op. Bij het gemeentehuis van Hasselt kom ik uit het water: hier is mijn moeder om mij op te halen. De tijd is net iets voor 8 uur ’s avonds… ik heb in totaal 11 uur mogen reizen voor dit moment… De dataEen kaart van mijn route is hieronder weergegeven. Elke marker geeft de kilometerstand aan. Halverwege de route begint deze opnieuw: ik heb hier voor het gemak een groene marker geplaatst om de overnachting aan te geven. Mijn gemiddelde snelheid op de eerste dag was 7.39 (bewogen gemiddelde) en 7.34 op de 2e dag. De totale afstand was 110.7 kilometer waarvan 50.5 op dag 1, en 60.2 op dag 2. In totaal was ik ongeveer 9 uur onderweg op dag 1, en 10:45 uur op dag 2. Mijn effectieve snelheid (dus tijd inclusief pauzes en overdragingen) was hiermee ongeveer 5.6 kilometer per uur- iets sneller dan het normale toertochttempo. Tot slotIk mag terugkijken op een leuke tocht die ik wonderlijk tussen het college-volgen door heb kunnen afleggen. Het is nu herfst en het weer wordt slechter, mijn rooster drukker en aan de horizon luiken de stages die ik moet afleggen ter voltooiing van mijn studie. Het is dus helemaal niet meer zeker dat ik dit jaar nog dergelijke avonturen kan doen. Ook de Selkie mag even bijkomen. Bij inspectie blijkt hij een paar flinke krassen en een knal te hebben opgelopen. Voor mijn volgende avontuur ga ik informatie inwinnen of ik dit moet of kan repareren. De boot is overigens niet lek. Zonder Ton’s hulp zou deze tocht niet zo snel doorgegaan zijn. Ton, super bedankt voor jouw raad en gastvrijheid! Tot bij het volgende kanoevenement en van harte gefeliciteerd met je 10.000 kilometer op de Vecht! Tot slot zou deze tocht niet mogelijk zijn geweest zonder mijn moeder die mij af kon zetten en op kon komen halen. Bedankt mam! :)","link":"/2017/09/27/De-Vechtrally/"},{"title":"Kanoën in de Onlanden","text":"De OnlandenBeschrijving Natuur Kaart Kanokaart Overstappen Om rekening mee te houden Verhuur in de buurt","link":"/2017/12/14/Kanoen-in-de-Onlanden/"},{"title":"Big Data Series - Hadoop and the HDFS","text":"HDFS and Map-ReduceThe general idea of this post is to work out a short hello-world type of tutorial. For convenience I’ll assume that you have some basic understanding of the idea behind Map-Reduce and why you’d ought to use it. For this post though, I’m not going to go with a very complicated use case, instead sticking to the most basic solution (also I had other deadlines to meet). Keep in mind that what goes for Shakespeare should also go for the 450-million in words COCA. Maybe something for next week, Arjen? :) Setting up the environmentSince I’m a stubborn old goat I’m running vagrant to run a virtual Ubuntu distribution. Inside of which I’m running a docker container as was requested per assignment. On my laptop, which runs Windows 8. Following the instructions I install hadoop version 2.7.3 and set up a hdfs cluster. There is some non-trivial path setting up involved here, but it goes beyond the scope of this assignment. Getting dataThe Complete Shakespeare corpus hosted by Project Gutenberg is only 5.3 MB. Not exactly big data. But for our little tutorial it’ll do just fine. For convenience I download it using wget… The Hello World ExampleConsidering everyone in the class had to do the same assignment I’m not going to take you by the hand to lead you through the entire Hello World Example on Hadoop again. However, a short summary is in place: The first steps are setting up the cluster. For the standalone version you don’t actually have to do much more. However, we can get Hadoop to run on a single node in a pseudo distributed manner. To first do this, we have to edit the xml config files found under etc/hadoop: etc/hadoop/core-site.xml: &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; etc/hadoop/hdfs-site.xml: &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; After that, we need to get hadoop to format a new HDFS: bin/hdfs namenode -format sbin/start-dfs.sh bin/hdfs dfs -mkdir -p /user/root Now, we pull the corpus and the java file: wget http://www.gutenberg.org/ebooks/100.txt.utf-8 wget https://gist.githubusercontent.com/WKuipers/87a1439b09d5477d21119abefdb84db0/raw/c327b9f74d30684b1ad2a0087a6de805503379d3/WordCount.java And make the directories we need plus the jar we’re going to run: bin/hdfs dfs -mkdir input bin/hdfs dfs -put 100.txt.utf-8 input bin/hadoop com.sun.tools.javac.Main WordCount.java jar cf wc.jar WordCount*.class Finally, we run the jar on the corpus: bin/hadoop jar wc.jar WordCount input output bin/hdfs dfs -get output/part-r-00000 bin/hdfs dfs -rm -r output We can inspect the results with Nano. This is a bit archaic, but what in computer science isn’t? nano part-r-00000 In the resulting file we have tuples of each token with the cumulative count of their occurence. Don’t be alarmed if it looks off: the script does not actually know that “Juliet.” and “Juliet?” refer to the same token. So how does Mapreduce really count these words?The WordCount java code contains a simpel flow consisting of a map-step that emits words (including special characters such as !?,.:) that are deliited by whitespace along with a count of how many times they were encountered. This processing is done one line at a time. In the reduce step each map is combined so that we get a nice hash map that sums up the values. Food for thought (or other questions..)So what happens when you just do the standalone part of the tutorial?A standalone operation just builds a single node. This is probably very handy for programming and debugging. In this mode the commands are handled by just a single node which is kind of defeating the purpose of hadoop! So what’s different in the pseudo-dist case?In the pseudo-distributed case we build a simulated cluster with multiple nodes. This would allow big tasks to be done in an asynchronous way. In the pseudo-distributed case I had to do more set-up and much more debugging. Really, the standalone variant and the simulated clusters just look like debugging settings for when you can’t or will not work on a real cluster. So who’s the most popular kid in town?So there are more ways to determine who’s more popular. If we count purely the amount of times Romeo or Juliet spoke, we just have to look for the lines starting with their name. For Romeo this is “Rom.” and he has 167 lines. Our tragic Juliet “Jul.” only has 117. But who gets the most mentions? “Juliet” is mentioned 68 times, while “Romeo” in all its forms gets mentioned 152 times. *Things aren’t always fair on fourteen year-olds. * Sometimes our brains are able to repress bad memories. However, it all came back to me when I saw the Java code… those long days in a hot and sweaty computer lab, trying to understand OO did however make me into the man I am.","link":"/2017/03/22/Hadoop-and-the-HDFS/"},{"title":"Mapping Gritters","text":"Last year, the Dutch Rijkswaterstaat (a part of the Dutch Ministry of Infrastructure and the Environment) released a website where you could track where salt scattering trucks - also known as gritters - moved in real-time. This is particularly useful as Dutch infrastructure always seems to shut down completely during the first days of mild snow and you need to know if there’s a chance you might make it to work today. The Rijkswaterstaat website features a Google Maps widget that shows which trucks are active and moving and which are not. Getting the dataThe website features an API, which while not publicly advertised can be found by opening dev tools in any modern browser and looking at the requests made by the page. This url can then be approached via the URL. Snooping around I found a nice stream of JSON data: 1{&quot;id&quot;:&quot;108191021&quot;,&quot;workcode_id&quot;:&quot;34&quot;,&quot;latitude&quot;:&quot;52.053938&quot;,&quot;longitude&quot;:&quot;5.115533&quot;} The response includes: coordinate pairs in latitude and longitude (in WGS84!) a truck identifier (id) a code whether the truck is active or not. (workcode_id) Visualizing the dataI quickly wrote a small script to download this JSON from their site - every sixty seconds or so - and started to build my own real-time overview of the gritter truck by converting the data to GeoJSON and plotting it in TNO’s Common Sense. This yielded the following image. At this point I could only see where they were located. If I wanted to get new data I would have to refresh my map, and I’d still be working with point data. I had to find a way to collect historical data of the trucks. A second version of the script used an array of past values to collect historical data. Every time the script requested the JSON file, I would add the new values to an array I’d connect to each truck, and save it on my local file system. With a little bit of extra code I converted these to GeoJSON LineStrings, which are essentially lines between each coordinate pair I received. Now I had a way to show where trucks had been moving. The following image is aggregated from a day worth of data: Not surprisingly, it only showed which roads were part of the Rijkswaterstaat’s responsibility: main high ways et cetera. What was kind of cool though was that you could see exactly where they were, including ramps. Wrapping upThat’s about it. I found the data, transformed it, and made a pretty image of it. Very little code had to be written thanks to the tools available. The code can be found on Github for anyone who wants to mess with it. *Update: The Rijkswaterstaat website has been updated and the API path has been changed. You can find the new JSON file here They also added a nice historical viewing option, which shows you where trucks have been in the past six hours. *","link":"/2016/01/16/Mapping-Gritters/"},{"title":"Kayaking: Appingedam in the fog (19km)","text":"Probably one of the last trips we’ll make this year was a 19km peddle around Appingedam. By no means this was a long trip not suited for beginners, but our group was small perhaps because of the weather. Temps had dropped to 6 degrees celcius, but not enough to make some people consider wearing neoprene pants. We started off right in the city center, paddling around the east bend before making our way north, out of the medieval city. While there we passed the famous hanging kitches, one of which seemed - amusingly - for sale. Apart from a couple of friendly fishermen we didn’t have to watch much: we were the only souls on the water. In the polders north of Appingedam we found dense fog, which made the trip a lot more mystique than it was. After 55 minutes we found our first resting point. Even though most of the group could have easily skipped the rest completely, no one complained about coffee. After embarking again we headed west, again through the foggy polder landscape. We paddled past farms and reedy canals. Eventually, after passing the site of the Dutch Oil Company (NAM) we found our second rest - a cafe turned into a restaurant where a single chef gladly made us coffee, chocolate, and apple pie. Our previous rest had been about an hour ago, so we didn’t really need to stop here either, but hey, coffee. Once we all had our lunch, we embarked again and went south. Our starting point was only about six kilometers away and we made it after about 45 minutes. And that was with me holding everyone up, as I felt like I couldn’t paddle faster as I had been a bit sick the weeks before and I was perhaps still slightly feverish. Common sense is not my strongest point. Our speed was perhaps a bit high for a beginners’ trip, with me being the least experienced person in the group. A lot of times we were comfortably hovering at 8km per hours or higher. In the end, we were on the water for 2 hours and 45 minutes. Our total distance covered was 19.33 kilometers.","link":"/2015/10/27/Kayaking-Appingedam-in-the-fog-19km/"},{"title":"Resources voor beginnende kajakkers","text":"Voor beginnende kanovaarders zijn het diploma Kanovaardigheid A (KVA) en de Kanoveiligheidstest (KVT) waarschijnlijk de eerste formele certificaten waar ze mee in aanraking komen. Deze certificaten komen neer op lijstjes met vaardigheden die gedemonstreerd moeten worden tijdens een examen of tijdens bepaalde meetmomenten gedurende een reeks cursusdagen. Ik vind het zelf altijd fijn om online goed rond te kijken naar filmpjes en plaatjes van technieken om de praktijk wat makkelijker te maken. Er is echters zo gigantisch veel materiaal online en ook zijn veel termen moeilijk te vertalen naar het Engels, zeker voor een beginner! Daarom dit artikel met een verzameling links, om een beetje wat extra informatie te geven. Ik vind het zelf leuk om dit soort lijsten te maken, en als iemand er wat aan heeft ben ik allang blij. KVA-techniekenVoor een overzicht van de eisen voor het diploma KVA kun je kijken op de site van het WSV. http://kano.watersporters.nl/opleidingen-en-vaardigheden/. Dit lijstje komt voort uit de informatie van 2015… er is naar mijn weten bij schrijven (zomer 2018) geen recentere informatie beschikbaar. Tillen van een KajakOpmerking: Over het algemeen is de basisregel: is hij niet van jou? Til hem dan met z’n tweeën! Later, als je een eigen boot hebt, kun je altijd capriolen uit gaan halen.https://www.youtube.com/watch?v=ExARpQEHaHQ Opmerking: het tillen van een volledig volgeladen kajak gaat meestal iets anders! In- en uitstappenInstappen vanaf een steigerhttps://www.kayakpaddling.net/nl/1-1Instappen vanaf de oeverhttps://www.kayakpaddling.net/nl/1-2 Voorwaarts Varenhttps://www.kayakpaddling.net/nl/2-2https://www.youtube.com/watch?v=pvi7rIlsNRY (SeaKayakingTV)https://www.youtube.com/watch?v=FIfx-QDuHKQ (Paddling.com / Gordon Brown) Achterwaarts Varenhttps://www.kayakpaddling.net/nl/2-7 Stoppenhttps://www.youtube.com/watch?v=kRUpSLUFMII Lage Steunhttps://www.kayakpaddling.net/nl/4-2https://www.youtube.com/watch?v=HscxpfkOLWk (Thierry Nijbroek)https://youtu.be/8cJVmYQnAXY?t=3m33s (vanaf 3:33 - Northseakayak / Dimitri)https://youtu.be/BfUKClZl2Vc (Adventure Kayak Magazine) Boogslag Voorwaartshttp://www.dajaks.nl/cgi-bin/dajaks.pl?command=pagina&amp;hoofmenu_id=5&amp;submenu=93 (Dajaks / Olaf van Broekhoven)https://www.kayakpaddling.net/nl/2-4 Boogslag Achterwaartshttps://www.kayakpaddling.net/nl/2-4 Dynamische Lage SteunStiekem de leukste techniek. Dit is eigenlijk gewoon de toepassing van de lage steun om extra support te krijgen terwijl je om je heen (lees: achterom) kijkt. Als je een beetje snelheid hebt kan je er meer steun uithalen. Probeer eens de lage steun toe te passen terwijl je vaart. kantel je peddelblad zodat het water onder het blad doorgaat en voel hoeveel steun je er krijgt.https://www.neckykayaks.com/tips_techniques/paddling_skills/low_brace/ (Necky Kayaks, maar gaat eigenlijk over een andere techniek) Opkantenhttps://www.kayakpaddling.net/nl/2-5https://www.youtube.com/watch?v=AT-73owZUeA Zijwaarts verplaatsen (trekslag)https://www.kayakpaddling.net/nl/2-6 Zijwaarts verplaatsen (Wrikslag en soms scullen genoemd)http://www.kvdolfijnen.nl/commissies-2/jeugd-2/technieken/zijwaarts-verplaatsen/ (Wrikslag EN trekslag, KV Dolfijnen)https://www.youtube.com/watch?v=0NbWDhmREgo (Paddling.com / Mike Aronoff) Achterstevenroerhttps://www.youtube.com/watch?v=t_yrg-xkRXM (SeaKayakingTV)https://www.kayakpaddling.net/nl/2-3 Opruimen en omgaan met een kajakNiet specifiek veel tips. Maak hem droog, leg polyethyleen (plastic) kayaks op een vlakke ondergrond om vervormen door warmte of gewicht te voorkomen. Dekluiken kan je losgooien omdat er nog wel eens knaagdieren door heen willen bijten op zoek naar eten. Plus: zo kan alles mooi opdrogen en luchten. Omslaan met spatzeilhttps://www.kayakpaddling.net/3-2https://www.youtube.com/watch?v=blJFzirv3-E (Mike Aronoff / Paddling.com) ZelfreddingEr zijn veel technieken die je hiervoor kan gebruiken. Ik heb ze onderdeeld in categoriën die echt niet bedoeld zijn als volledige lijst: Paddle-floats of peddel als steunpunt:https://www.youtube.com/watch?v=hkj2S4yxoQI (Gordon Brown / Paddling.com)https://www.youtube.com/watch?v=sFrwEV7aMhc (Dimitri / Northseakayak)https://www.youtube.com/watch?v=_LLNATL6BdQ (Dimitri / Northseakayak) Cowboy Scramble:https://www.youtube.com/watch?v=2L_8ctRoMcM (eastcoastkayaking)https://www.youtube.com/watch?v=4bupgSO1XBE (Ontario Sea Kayak Centre) The ladder (let op: hierbij wordt de peddel gebruikt als kantelpunt)https://www.youtube.com/watch?v=2L_8ctRoMcM Re-entryhttps://www.youtube.com/watch?v=zm6wHEAiN8whttps://www.youtube.com/watch?v=dov8XkaXRVQ Geassisteerde ReddingKies er een! De meeste clubs hebben een voorkeur voor de hielhaak of de X-redding. Meestal is de X-redding makkelijker voor jonge mensen. Hielhaakhttps://www.youtube.com/watch?v=j-zpJQeiaNc (Gordon Brown / Paddling.com)https://www.youtube.com/watch?v=cap1wHLDOW4 X-reddinghttps://www.youtube.com/watch?v=vdne58uCtrMhttps://www.youtube.com/watch?v=e_oqxQqWPHM (Groninger Kanovereniging) T-redding (puntjesredding)https://www.youtube.com/watch?v=OiBd2RnXu7g (NRS) Scoop-rescuehttps://www.youtube.com/watch?v=wE5y_DW2h04 (Body Boat Blade) Theorie Uitleg over verschillende kano’s Uitleg over de onderdelen van een kanoplaatje https://www.kayakpaddling.net/nl/1-4 Uitleg over de verschillende peddels Verschil reddings- en zwemvesthttps://www.kanoshop.nl/advies-uitrusting Voeding Onderkoeling (KVA) Kano-organisaties en opleidingsmogelijkheden Kleding Basisvaarregels KVT techniekenKVT wordt al wat moeilijker en serieuzer. EHBO wordt er bij betrokken en het wordt aangenomen dat je redelijk wat oefening hebt gehad in een groot aantal technieken. Helaas.. misschien kwam je zonder extra oefening door je KVA maar nu zul je toch echt dingen moeten gaan oefenen! Uitrusting Over sleeplijnen Over werplijnen Over de EHBO-set Over reddinszakken Het nut van warme drank PraktijkEHBO Stabiele zijligging Wat te doen bij onderkoeling Wat te doen bij braken Oververhitting bij watersporters Zwemmers in paniek Herkennen van shock Reddingen en natte praktijken Zwemmen met de kano X-Redding / Hielhaak/ Kiwi-methodehttp://www.dajaks.nl/cgi-bin/dajaks.pl?command=pagina&amp;hoofmenu_id=5&amp;submenu=448 (Dajaks, Max) Eskimo-redding / Punt-redding Zelfredding 2.0 Uitstappen zodat de kano rechtop blijft All-in Redding Duwsleep (contact) Slepen Werplijn gooien Overige dingen (niet verbonden aan een bepaald diploma)Alles gaat op rolletjes Sweep rolls C to C roll Re-entries Cowboy Scramble","link":"/2017/11/17/Resources-voor-beginnende-kajakkers/"},{"title":"Site Revamp","text":"New looksI’ve reworked my personal website a bit. The theme I used already took a fair bit of work from my hands, but I spent about six hours modifying it and ironing out a few kinks. Just in time for the start of the academic year I hope to keep this updated with new projects and adventures. I also added disqus support so each article can now be commented upon. I had wanted to do this earlier with my big data mini blog series, but time wasn’t really available for shenanigans. BTW: The header image rotates between a couple of kayaking pics.","link":"/2017/09/03/Site-Revamp/"},{"title":"Big Data Series - SurfSara and the Common Crawl","text":"Part 1 - The Big Data Final ProjectThis post will have a slightly different angle than the previous posts in the Big Data Course series. The goal for this post is just to detail my progress on a self-chosen, free format project which utilizes the Surfsara Hadoop cluster and the goal is not to solve a problem but rather give an overview of the problems I encountered and the little things I came up with. I intend to post these both on the mini-site for the course and a personal blog, my apologies if my tone is a bit bland as a result. Here we go! Hathi and SurfsaraSurfSara is a Dutch institute that provides web and data services to universities and schools. Students may know SURF from the cheap software or the internet they provide to high schools. Sara, though is the high performance computing department, and used to be the academic center for computing prior to merging into SURF. They do a lot of cool things with big data which over time has come to include a Hadoop cluster named Hathi. The Common CrawlThe Hathi cluster hosts a February 2016 collection from the Common Crawl. The Common Crawl is a collection of crawled web pages which comes pretty close to crawling the entirety of the web. The data hosted is in the petabyte range, however we only have access to a single snapshot.. which still takes up a good amount of terabytes and contains 1.73 billion urls. You don’t want to download this on your mobile phone’s data cap. The Common Crawl Data is stored in WARC files (Web Archive), an open-source format. So with all this data, there should be a lot of things to do! Some ideas I had at this point: Count the length of all payloads across all pages on the internet and get some statistics. See how popular certain HTML tags are. Perform some semantic analysis on pages referring to presidents or politics. Look at how extreme right communities differ from extreme left communities in terms of vocabularies and word frequencies. Similarly, compare places like 4chan and Reddit with each other. Who’s more vile? There’s some easy libraries for sentiment analysis.. And so on.. but what I also played with is something closer to home. I kayak a lot and the kayaking community in the Netherlands is slowly dying: young people are turning away from adventurous sports in general, and kayaking is seen as boring when compared to other, fast-paced water sports (Not necessarily true, but still). Could I try to find places where it’s worthwhile to advertise about kayaking perhaps? Or identify communities of people who also kayak, e.g. mountain bikers, sailers, bikers etc? Or perhaps from another perspective, can I try to do some dynamic filtering based on brands or parts of the sport to see what people associate it with? Plenty of ideas, so let’s get started. Part 2 - Setting upI’m using my Windows laptop running a (Ubuntu) virtual machine which will be used to connect to SurfSara and develop the code. Similarly to the previous assignment in this series this works with a docker image and lots of command line work. Nothing to be scared of. Running an example program worked fine on the cluster. But I wanted something more than (redirectable) output in my terminal. In order to track the jobs given to Hathi a web interface is available. This is not really supported on Windows, but still doable. Using the Heimdall implementation of Kerberos and the Identity Manager I can set up my credentials. I found that I needed to stray from the sort-of specific instructions courtesy of the Uni of Edinbourgh here and actually ended up installing the Heimdall tools fully. I then had to tweak a couple of configurations inside my firefox browser in order to work with Kerberos, but I finally could inspect the progress of my submissions. This seems easy, but in the end was a non-trivial part that took hours to do and even then Firefox was prone to memory leaks. Part 3 - A local testI started working with the spark notebook that was provided and after some tweaking around I could run code on a local WARC file containing the course website. This was an iterative process: I started with the grand idea of what I could do but after a few hours I found that I still had made no progress. Following Arjen’s suggestion of settling for a simpler challenge when stuck I tried to implement the most basic word count. This was OK-ish, and could be expanded to the full crawl albeit a bit sluggish (slow), which would be decent towards meeting the assignment criteria but I’ll let you be the judge of that. I also avoided SQL this time, as I recall reading that there are some issues when running SQL-queries on something of the order of 100TB. This could complicate things considering our ‘stack’ already consists of so many applications and tools. Additionally, having worked with MySQL as a teenager I’m still pretty sure that straight up SQL queries on non-indexed text fields is a baaaad idea. I felt like I still really wanted to do more with the kayaking thing. After some pondering I settled on the following order of battle: Convert the crawl to text and look for the string kayaking For the full crawl: figure out how to filter for a specific brand (e.g. bever) Construct a word count list upon the pages that get returned Output these to a file so I can work with it Visualize a word cloud, e.g. using the d3.js method already readily available, or something in python (This is outside the scope of this assignment, and I’ll add it later) So I started with filtering for the text string kayaking after calling Arjen’s HTML2Text method (step 1). 123map(wr =&gt; HTML2Txt(getContent(wr._2))).map(w =&gt; w.toLowerCase()).filter(w =&gt; w.contains(&quot;kayaking&quot;)). Now on the basic corpus this returns my own page obviously, as I overshared my love for kayaking a fair bit. As per Apache Spark’s example a word count is implemented with just a few lines of code (step 2): 123textFile.flatMap(line =&gt; line.split(&quot; &quot;)) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) This is a bit crude, as the “words” will include code snippets like the one found on this blog, and random gibberish like solitary punctuation marks. For a full pass over the crawl though I don’t think it’ll matter, as full words will drown out the noise. So now I’ve got a big old list of words with counts. Can I save this? Locally, I can use: warcl.saveAsTextFile(&quot;testje.txt&quot;) I’m guessing this will be different for the full crawl but one problem at a time. This creates a folder (!) with several files: output can be found in one file here. It’s interesting that everything to save into a text file was done below the hood without a warning being thrown at my face for not saving to a hdfs! There are some caveats with this: In the presentations some people noticed an integer overflow when using word counts, can I figure out something for this? I need to filter out common words such as “a”, “the” and so on. I can do this at a high level or when making the visualisation later on. Will save the problem for now.. Between the docker container and my Ubuntu host I found that I can copy files using docker cp. What if my files are big, though? And what happens on the full crawl. Write to standard output and just do everything on the cluster? May I need to purge tags and code from my result file? How can I easily scale this up to.. say looking at 20 brands at once? As was shown in the terminal above, it doesn’t make sense yet to construct a word cloud from a single page, I suppose though that the same steps go for the full cluster. Let’s move on and see if we can export the code to the full crawl! Part 4 - From Concept to ClusterThe following section will detail the process I went through when exporting the app to the cluster. Attempt 1 - Top 300 words over all sites containing “Kayaking”The first attempt is going to go over the entire crawl and look for the term ‘kayaking’ amongst the payload of all sites. I see some potential issues with this.. mainly because I’m asking for the entire crawl to be parsed through html2text - I reckon that is going to be an immense bottleneck. The core idea is explained in the following two code snippets.. 1234567891011val warcc = warcf. filter{ _._2.header.warcTypeIdx == 2 /* response */ }. filter{ _._2.getHttpHeader().statusCode == 200 }. filter{ _._2.getHttpHeader().contentType != null }. filter{ _._2.getHttpHeader().contentType.startsWith(&quot;text/html&quot;) } .map(wr =&gt; HTML2Txt(getContent(wr._2))) .map(w =&gt; w.replaceAll(&quot;[?!,.\\&quot;-()]&quot;, &quot;&quot;)) .map(w =&gt; w.toLowerCase()) .filter(_ != &quot;&quot;) .filter(w =&gt; w.contains(&quot;kayaking&quot;)) .cache() The above snippet checks for non-empty input and skips it. It should be refactored, but I’m still working on more ideas so I felt it should not be a big priority right now. It also checks for odd characters, e.g. ?! et cetera- we don’t want any of that sillyness. Finally, this big pile of text needs to get filtered for the phrase kayaking - I expect this line just after HTML2Txt to be a huge bottleneck. The next snippet does the standard MR word count. I’ve added a sort and top-300 selection. 12345678//now to construct a list with anything in the pages we have found per spark word count example https://spark.apache.org/examples.htmlval warcl = warcc.flatMap(_.split(&quot; &quot;)) .filter(_ != &quot;&quot;) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) .filter( w =&gt; !(commonWords.contains(w._1))) .sortBy(w =&gt; -w._2) .take(300) Lastly, this gets printed to the output. Filtering for Common wordsI browsed around for a solution to the common word problem, as I didn’t feel like editing my top 300 list every time. So I found this stackoverflow question about filtering words out of my input, by means of a sequence. So I still needed a sequence at that point, and I found this list of English stop words.. which brings me to wonder if I’m going to see other languages pop to the top of the list. One problem at a time though. For clarity, here’s the complete list. 1val commonWords = Set(&quot;a&quot;, &quot;about&quot;, &quot;above&quot;, &quot;above&quot;, &quot;across&quot;, &quot;after&quot;, &quot;afterwards&quot;, &quot;again&quot;, &quot;against&quot;, &quot;all&quot;, &quot;almost&quot;, &quot;alone&quot;, &quot;along&quot;, &quot;already&quot;, &quot;also&quot;,&quot;although&quot;,&quot;always&quot;,&quot;am&quot;,&quot;among&quot;, &quot;amongst&quot;, &quot;amoungst&quot;, &quot;amount&quot;, &quot;an&quot;, &quot;and&quot;, &quot;another&quot;, &quot;any&quot;,&quot;anyhow&quot;,&quot;anyone&quot;,&quot;anything&quot;,&quot;anyway&quot;, &quot;anywhere&quot;, &quot;are&quot;, &quot;around&quot;, &quot;as&quot;, &quot;at&quot;, &quot;back&quot;,&quot;be&quot;,&quot;became&quot;, &quot;because&quot;,&quot;become&quot;,&quot;becomes&quot;, &quot;becoming&quot;, &quot;been&quot;, &quot;before&quot;, &quot;beforehand&quot;, &quot;behind&quot;, &quot;being&quot;, &quot;below&quot;, &quot;beside&quot;, &quot;besides&quot;, &quot;between&quot;, &quot;beyond&quot;, &quot;bill&quot;, &quot;both&quot;, &quot;bottom&quot;,&quot;but&quot;, &quot;by&quot;, &quot;call&quot;, &quot;can&quot;, &quot;cannot&quot;, &quot;cant&quot;, &quot;co&quot;, &quot;con&quot;, &quot;could&quot;, &quot;couldnt&quot;, &quot;de&quot;, &quot;describe&quot;, &quot;detail&quot;, &quot;do&quot;, &quot;done&quot;, &quot;down&quot;, &quot;due&quot;, &quot;during&quot;, &quot;each&quot;, &quot;eg&quot;, &quot;eight&quot;, &quot;either&quot;, &quot;eleven&quot;,&quot;else&quot;, &quot;elsewhere&quot;, &quot;empty&quot;, &quot;enough&quot;, &quot;etc&quot;, &quot;even&quot;, &quot;ever&quot;, &quot;every&quot;, &quot;everyone&quot;, &quot;everything&quot;, &quot;everywhere&quot;, &quot;except&quot;, &quot;few&quot;, &quot;fifteen&quot;, &quot;fify&quot;, &quot;fill&quot;, &quot;find&quot;, &quot;five&quot;, &quot;for&quot;, &quot;former&quot;, &quot;formerly&quot;, &quot;forty&quot;, &quot;found&quot;, &quot;four&quot;, &quot;from&quot;, &quot;front&quot;, &quot;full&quot;, &quot;further&quot;, &quot;get&quot;, &quot;give&quot;, &quot;go&quot;, &quot;had&quot;, &quot;has&quot;, &quot;hasnt&quot;, &quot;have&quot;, &quot;he&quot;, &quot;hence&quot;, &quot;her&quot;, &quot;here&quot;, &quot;hereafter&quot;, &quot;hereby&quot;, &quot;herein&quot;, &quot;hereupon&quot;, &quot;hers&quot;, &quot;herself&quot;, &quot;him&quot;, &quot;himself&quot;, &quot;his&quot;, &quot;how&quot;, &quot;however&quot;, &quot;hundred&quot;, &quot;ie&quot;, &quot;if&quot;, &quot;in&quot;, &quot;inc&quot;, &quot;indeed&quot;, &quot;into&quot;, &quot;is&quot;, &quot;it&quot;, &quot;its&quot;, &quot;itself&quot;, &quot;keep&quot;, &quot;last&quot;, &quot;latter&quot;, &quot;latterly&quot;, &quot;least&quot;, &quot;less&quot;, &quot;ltd&quot;, &quot;made&quot;, &quot;many&quot;, &quot;may&quot;, &quot;me&quot;, &quot;meanwhile&quot;, &quot;might&quot;, &quot;mill&quot;, &quot;mine&quot;, &quot;more&quot;, &quot;moreover&quot;, &quot;most&quot;, &quot;mostly&quot;, &quot;move&quot;, &quot;much&quot;, &quot;must&quot;, &quot;my&quot;, &quot;myself&quot;, &quot;name&quot;, &quot;namely&quot;, &quot;neither&quot;, &quot;never&quot;, &quot;nevertheless&quot;, &quot;next&quot;, &quot;nine&quot;, &quot;no&quot;, &quot;nobody&quot;, &quot;none&quot;, &quot;noone&quot;, &quot;nor&quot;, &quot;not&quot;, &quot;nothing&quot;, &quot;now&quot;, &quot;nowhere&quot;, &quot;of&quot;, &quot;off&quot;, &quot;often&quot;, &quot;on&quot;, &quot;once&quot;, &quot;one&quot;, &quot;only&quot;, &quot;onto&quot;, &quot;or&quot;, &quot;other&quot;, &quot;others&quot;, &quot;otherwise&quot;, &quot;our&quot;, &quot;ours&quot;, &quot;ourselves&quot;, &quot;out&quot;, &quot;over&quot;, &quot;own&quot;,&quot;part&quot;, &quot;per&quot;, &quot;perhaps&quot;, &quot;please&quot;, &quot;put&quot;, &quot;rather&quot;, &quot;re&quot;, &quot;same&quot;, &quot;see&quot;, &quot;seem&quot;, &quot;seemed&quot;, &quot;seeming&quot;, &quot;seems&quot;, &quot;serious&quot;, &quot;several&quot;, &quot;she&quot;, &quot;should&quot;, &quot;show&quot;, &quot;side&quot;, &quot;since&quot;, &quot;sincere&quot;, &quot;six&quot;, &quot;sixty&quot;, &quot;so&quot;, &quot;some&quot;, &quot;somehow&quot;, &quot;someone&quot;, &quot;something&quot;, &quot;sometime&quot;, &quot;sometimes&quot;, &quot;somewhere&quot;, &quot;still&quot;, &quot;such&quot;, &quot;system&quot;, &quot;take&quot;, &quot;ten&quot;, &quot;than&quot;, &quot;that&quot;, &quot;the&quot;, &quot;their&quot;, &quot;them&quot;, &quot;themselves&quot;, &quot;then&quot;, &quot;thence&quot;, &quot;there&quot;, &quot;thereafter&quot;, &quot;thereby&quot;, &quot;therefore&quot;, &quot;therein&quot;, &quot;thereupon&quot;, &quot;these&quot;, &quot;they&quot;, &quot;third&quot;, &quot;this&quot;, &quot;those&quot;, &quot;though&quot;, &quot;three&quot;, &quot;through&quot;, &quot;throughout&quot;, &quot;thru&quot;, &quot;thus&quot;, &quot;to&quot;, &quot;together&quot;, &quot;too&quot;, &quot;top&quot;, &quot;toward&quot;, &quot;towards&quot;, &quot;twelve&quot;, &quot;twenty&quot;, &quot;two&quot;, &quot;un&quot;, &quot;under&quot;, &quot;until&quot;, &quot;up&quot;, &quot;upon&quot;, &quot;us&quot;, &quot;very&quot;, &quot;via&quot;, &quot;was&quot;, &quot;we&quot;, &quot;well&quot;, &quot;were&quot;, &quot;what&quot;, &quot;whatever&quot;, &quot;when&quot;, &quot;whence&quot;, &quot;whenever&quot;, &quot;where&quot;, &quot;whereafter&quot;, &quot;whereas&quot;, &quot;whereby&quot;, &quot;wherein&quot;, &quot;whereupon&quot;, &quot;wherever&quot;, &quot;whether&quot;, &quot;which&quot;, &quot;while&quot;, &quot;who&quot;, &quot;whoever&quot;, &quot;whole&quot;, &quot;whom&quot;, &quot;whose&quot;, &quot;why&quot;, &quot;will&quot;, &quot;with&quot;, &quot;within&quot;, &quot;without&quot;, &quot;would&quot;, &quot;yet&quot;, &quot;you&quot;, &quot;your&quot;, &quot;yours&quot;, &quot;yourself&quot;, &quot;yourselves&quot;, &quot;the&quot;) Getting the code to HathiSo right now I have a very basic and simple example Scala app which is confined to the notebook. I still need to do some house keeping in order to get it on the cluster. The first step is exporting it to scala. This opens the file in my browser. I stored the file in a public location on the web (so I could get it via wget from the docker and pushed my updates to it - this allowed me to edit the file using the tools on my own machine and pull it when I want to run it on the cluster. This greatly reduced my effort by reducing my dependency on tools like vim - which, while excellent, do not have the range of capabilities like atom or VS Code do. Again, personal preference. I then used the skeleton from the example app on the hathi-surfsara image, replacing the original file and deleting the /target/ folder. I made sure to follow the steps needed in the creating a self-contained app tutorial: which meant stripping some code and defining a main method. Additionally, I added jsoup to the libraryDependencies. Using sbt assembly I then created a fat jar (stored in /target/) and submitted it via spark-submit --master yarn --deploy-mode cluster --num-executors 300 --class org.rubigdata.RUBigDataApp /hathi-client/spark/rubigdata/target/scala-2.11/RUBigDataApp-assembly-1.0.jar So for the next 1 minute 40 seconds I was thrilled! Hathi picked up my submission and seemed happy to do it. Then I got a nullPointerException.. turned out I was checking for the contentType before even checking if this wasn’t null instead of the other way around.. eager beaver. I had the bright idea to implement a check for it, but did so in the wrong order. The next big error was regarding my use of saveAsTextFile. Because this would be called many times (once per warc file?) I would get the error that the folder already existed. I took the saveAsTextFile out, and redirected output about the top 300 to the stdout instead. After this small fix the code was submitted and I went to bed.. After 8 hours, 36 minutes and 45 seconds my code apparently hit an error: potentially having to do with a block being unavailable on the cluster. Just as I was rolling over hugging my pillow, the little cluster named Hathi was in tears. Had it failed the user, or had the user failed it? 123456User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1263 in stage 0.0 failed 4 times, most recent failure: Lost task 1263.3 in stage 0.0 (TID 3214, worker168.hathi.surfsara.nl, executor 256): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-16922093-145.100.41.3-1392681459262:blk_1262268700_188594112 file=/data/public/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701146196.88/warc/CC-MAIN-20160205193906-00216-ip-10-236-182-209.ec2.internal.warc.gzat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:945) at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:604) at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:844) at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:896)at java.io.DataInputStream.read(DataInputStream.java:149) I tried to google the error, but found nothing that I could do as a normal user of the cluster. Most of these had to do with missing privs (might be possible) or corruption. Link to the application details I’ve posted an issue, meanwhile I’m going to run it on a single warc segment. Attempt 2 - One segmentUsing the index I’ve looked for https://www.ukriversguidebook.co.uk/ - a large internet community of kayakers. This gives me a neat JSON output containing the locations of all hits. I just picked one- and added it to my code as &quot;/data/public/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148402.62/*&quot;. The rest of the code remained unchanged for the reproducibility of errors. I submitted it with 300 executors and went to get a shave. 15 minutes and 15 seconds later the submission was done, much to my surprise. I had covered 698 tasks. Bear in mind this submission was 1% of the entire crawl, and I stomped through with 300 executors. No error was given, and my glorious output was waiting for me. The following screenshot shows the inside of the Applicationmanager just after starting. Honestly, glancing over this felt like being inside mission control at NASA. Now the curious reader will want to know.. what did we get from this? Earlier I redirected output to stdout: this is where my little frequency list ended up. 12345678910111213141516womens;849513;ski;774168;jackets;738558;-;703664; ;673135;clothing;614007;snowboard;581317;accessories;492069;pants;475875;bike;467037;bikes;463572;mens;460966;bags;456382;shop;430335;sunglasses;426637;shoes;423502; There’s still some noise. Apparently I missed a white space and ‘-‘… oh well. This list seems to indicate that most websites referring to kayaking sell clothing and gear for outdoor activities. That makes sense, given that this is a huge industry with many competitors. Perhaps it would be a good idea to create a second list with words common in retail. It’s interesting that words like sea and nature don’t appear at all. The word safety - which is at the heart of the sport is ranked #273. Perhaps this is just a batch with a lot of retail sites, but it seems like a decent idea to mine retail terms in order to filter them out for the next iteration. So I started to work and added another 150 words to the list with all those retail phrases. I refined the method and submitted the jar once more. Nothing was really different apart from a little retouching. Again, the code worked fine and I got a new list! I then wrote a little bit of javascript to convert the frequency list I had to a payload that could be used for a word cloud (credits: https://github.com/wvengen/d3-wordcloud) and generated the visualization. The word cloud is pretty cool. Most of the junk has been filtered and we see a lot of sports and outdoor-related terms. I guess that the market for kayaking is the same as the market for bikes and wakeboarding. As a mountain biker myself this is amusing. It also shows Wisconsin. This might be random, but the American state also borders lake Michigan and other large lakes and rivers. Attempt 3 - Selective filtering, and finding brands!Lastly I wanted to filter this subsection for specific brands. While I could easily create a list of 50 or so brands of varying popularity I chose Rockpool. Rockpool is a manufacturer of sea kayaks with several models being extremely popular in the expedition kayaking scene. In a year or so, when I graduate.. you can pretty much guess where my pay check is going to. Look at this boat! Jokes aside, let’s find the same word list as for kayaking. I added a brands set at first, but that didn’t work out quite well. While I could iterate through it with the following code.. 12345678910111213val brands = Set(&quot;p&amp;h&quot;, &quot;valley&quot;, &quot;rockpool&quot;, &quot;peakuk&quot;)for (brand &lt;- brands) { println(brand+&quot;\\n&quot;) val warcd = warcc /*temp*/ warcd.cache() /*lazy evaluation*/ val warcl = warcd.filter(w =&gt; w.contains(brand)) .flatMap(_.split(&quot; &quot;)) .filter(_ != &quot;&quot;) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) .filter( w =&gt; !(commonWords.contains(w._1))) .sortBy(w =&gt; -w._2) .take(300) I would continously narrow down my collection. E.g. the first brand would go fine, but the second brand would be filtered from the subsection of the first brand and so on. This is due to Spark’s Lazy Evaluation^tm where nothing is actually executed until a reduce operation- and in my code I only used reduceByKey until the end of each brand-specific execution. Regardless, being my favourite kayak manufacturer I chose Rockpool and got the following list: 1234567891011121314151617181920212223242526272829303132{text: &apos;nov&apos;, size: 439},{text: &apos;mar&apos;, size: 407},{text: &apos;jan&apos;, size: 382},{text: &apos;dec&apos;, size: 379},{text: &apos;apr&apos;, size: 369},{text: &apos;feb&apos;, size: 363},{text: &apos;oct&apos;, size: 362},{text: &apos;jul&apos;, size: 355},{text: &apos;jun&apos;, size: 352},{text: &apos;sep&apos;, size: 338},{text: &apos;2006&apos;, size: 282},{text: &apos;aug&apos;, size: 265},{text: &apos;ago&apos;, size: 216},{text: &apos;stay&apos;, size: 214},{text: &apos;cottage&apos;, size: 214},{text: &apos;13&apos;, size: 202},{text: &apos;loch&apos;, size: 201},{text: &apos;12&apos;, size: 199},{text: &apos;16&apos;, size: 194},{text: &apos;holiday&apos;, size: 194},{text: &apos;house&apos;, size: 193},{text: &apos;21&apos;, size: 189},{text: &apos;11&apos;, size: 188},{text: &apos;17&apos;, size: 188},{text: &apos;20&apos;, size: 186},{text: &apos;22&apos;, size: 185},{text: &apos;15&apos;, size: 181},{text: &apos;14&apos;, size: 179},{text: &apos;27&apos;, size: 176},{text: &apos;19&apos;, size: 174},{text: &apos;night&apos;, size: 174},{text: &apos;28&apos;, size: 169}, While some words are close (e.g. loch) it seems we picked up a lot of calendar or blog contents. After some manual (I’m not going to run this on the cluster and wait another 20 minutes) removal of the nonsense I got the following list: 1234567891011121314151617181920212223{text: &apos;ago&apos;, size: 216},{text: &apos;stay&apos;, size: 214},{text: &apos;cottage&apos;, size: 214},{text: &apos;loch&apos;, size: 201},{text: &apos;16&apos;, size: 194},{text: &apos;holiday&apos;, size: 194},{text: &apos;house&apos;, size: 193},{text: &apos;night&apos;, size: 174},{text: &apos;home&apos;, size: 167},{text: &apos;details&apos;, size: 165},{text: &apos;18&apos;, size: 157},{text: &apos;view&apos;, size: 156},{text: &apos;min&apos;, size: 155},{text: &apos;book&apos;, size: 155},{text: &apos;great&apos;, size: 137},{text: &apos;views&apos;, size: 137},{text: &apos;away&apos;, size: 127},{text: &apos;sea&apos;, size: 126},{text: &apos;reviews&apos;, size: 125},{text: &apos;close&apos;, size: 118},{text: &apos;years&apos;, size: 117},{text: &apos;sleeps&apos;, size: 114},{text: &apos;5/5&apos;, size: 110}, This is more like it. I kept the 16 and 18 as they are both kayak models. Overall, I pruned about 50 words- I might add a regular expression on my next run on the cluster. However, something like 5/5 (a rating, included in the list above) might get lost unintentionally. The word cloud on ‘Rockpool’ is as follows: The only downside to this is the small corpus I get. Even though I used 1% of the common crawl, most of the words appear about 200 times. I wish I could run it again to get more data, but I do not want to drain up the entire cluster for a entire day. EDIT: Full crawl!I re-submitted the first job that went over the entire crawl. This time I used the retailWords list, as well as filtering for pages that also contained the word sea. I opted to get the top 1000 words instead. The submission was succesful and ended after 10hrs, 47mins, 12sec. In total 69800 jobs were queued. The top 20 words on the entire crawl are: 1234567891011121314151617181920&apos;snowboard&apos; 58172829&apos;accessories&apos; 47357717&apos;bike&apos; 46462542&apos;bikes&apos; 45888592&apos;shop&apos; 40646038&apos;country&apos; 28916820&apos;water&apos; 27793604&apos;cross&apos; 26669810&apos;casual&apos; 26185575&apos;gear&apos; 22752323&apos;wisconsin&apos; 21410085&apos;wakeboard&apos; 21104545&apos;travel&apos; 19564475&apos;packages&apos; 18880893&apos;hiking&apos; 18660510&apos;forum&apos; 17253484&apos;helmets&apos; 16881583&apos;royal&apos; 16747143&apos;bindings&apos; 16617302&apos;house&apos; 15067835 And the resulting word cloud is as follows: That concludes this blog post! Part 5 - EvaluationIn the above post I walked you through my adventures with the Common Crawl and the Dutch National Hathi Hadoop Cluster. I started off with basic examples and tried to solve my own problems as I went. Eventually I formed the idea of generating a word cloud based on the term kayaking. When it apparently was not possible to make a pass over the entire crawl I grabbed a 900 GB partition and worked with 1% of the data. My idea was still to look for how individual brands are viewed: e.g. what words are asociated with brands like Rockpool? Finally, I used javascript and the d3.js library to generate word clouds of my findings. Though I feel like I had to water down my challenges I feel like there’s a lot of things that I can still do with all this data. I’m still in unfamiliar territory and I learnt more each time. I’m still working on this project and I’d like to continue building a few interesting vizualisations. I’m glad I didn’t do the standard project, and it just feels better to try out many different things and get something of yourself out of a project like this. Overal I spent about 40 hours or so on this project. Dear reader, I thank you for your interest in this blog. Part 6 - Course EvaluationThough I already submitted the course evaluation I felt like it would be nice to include a few words on the process I went through for this course. I feel that using git, github and in particular the github pages tool - were enriching and powerful. I’m planning on including this repo with my own website, although I havent updated the latter in over two years. If I was a 2nd or 3rd-year student however, this would have instantly given me a portfolio of sorts which is incredibly useful to have. The way we went about it, trying to document our struggles with the various tools as we go is much closer to reality than just handing in a polished report that gets written after the product is already done. I would have liked more structure up front to combat the hours of troubleshooting I suppose, but in the end it turned out fine with just the support I found in the issue tracker and google. Speaking of the issue tracker, I think this was a great addition to the course and I hope it gets included for the students next year. It certainly helped a lot, and it breaks down the hurdles for students to step out and ask for help. I would advise to keep using it! // Jeffrey","link":"/2017/07/07/SurfSara-and-the-CommonCrawl/"}],"tags":[{"name":"node","slug":"node","link":"/tags/node/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"University","slug":"University","link":"/tags/University/"},{"name":"Assignment","slug":"Assignment","link":"/tags/Assignment/"},{"name":"Programming","slug":"Programming","link":"/tags/Programming/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"HDFS","slug":"HDFS","link":"/tags/HDFS/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"},{"name":"Kayaking","slug":"Kayaking","link":"/tags/Kayaking/"},{"name":"Vecht","slug":"Vecht","link":"/tags/Vecht/"},{"name":"Camping","slug":"Camping","link":"/tags/Camping/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"Common Sense","slug":"Common-Sense","link":"/tags/Common-Sense/"},{"name":"Data","slug":"Data","link":"/tags/Data/"},{"name":"Appingedam","slug":"Appingedam","link":"/tags/Appingedam/"},{"name":"Beginners","slug":"Beginners","link":"/tags/Beginners/"},{"name":"KVT","slug":"KVT","link":"/tags/KVT/"},{"name":"KVA","slug":"KVA","link":"/tags/KVA/"},{"name":"Watersportverbond","slug":"Watersportverbond","link":"/tags/Watersportverbond/"},{"name":"Site","slug":"Site","link":"/tags/Site/"},{"name":"SurfSara","slug":"SurfSara","link":"/tags/SurfSara/"},{"name":"Common Crawl","slug":"Common-Crawl","link":"/tags/Common-Crawl/"}],"categories":[{"name":"Programming","slug":"Programming","link":"/categories/Programming/"},{"name":"University","slug":"University","link":"/categories/University/"},{"name":"Assignment","slug":"Assignment","link":"/categories/Assignment/"},{"name":"Kayaking","slug":"Kayaking","link":"/categories/Kayaking/"},{"name":"Website","slug":"Website","link":"/categories/Website/"},{"name":"Assignment","slug":"University/Assignment","link":"/categories/University/Assignment/"},{"name":"Big Data Series","slug":"Assignment/Big-Data-Series","link":"/categories/Assignment/Big-Data-Series/"}]}